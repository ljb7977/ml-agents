{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 1e7 # Set maximum number of steps to run environment.\n",
    "run_path = \"Ball3\" # The sub-directory name for model and summary statistics\n",
    "load_model = True # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 1000 # Frequency at which to save training statistics.\n",
    "save_freq = 10000 # Frequency at which to save model.\n",
    "env_name = \"Ball2\" # Name of the training environment file.\n",
    "curriculum_file = None\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-4 # Strength of entropy regularization\n",
    "num_epoch = 5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 2 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 2048 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 1e-4 # Model learning rate.\n",
    "hidden_units = 64 # Number of units in hidden layer.\n",
    "batch_size = 64 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Ball3DAcademy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Ball3DAcademy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: Ball3DBrain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 14\n",
      "        Action space type: continuous\n",
      "        Action space size (per agent): 2\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file, worker_id=5)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./models/Ball3\\model-160000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/Ball3\\model-160000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Step: 161000. Mean Reward: 40.15652173913009. Std of Reward: 41.87743842179486.\n",
      "Step: 162000. Mean Reward: 34.91111111111081. Std of Reward: 40.96739745603746.\n",
      "Step: 163000. Mean Reward: 37.40689655172377. Std of Reward: 43.03975652919864.\n",
      "Step: 164000. Mean Reward: 21.18793103448259. Std of Reward: 35.214828761260115.\n",
      "Step: 165000. Mean Reward: 40.28666666666617. Std of Reward: 46.24907014080204.\n",
      "Step: 166000. Mean Reward: 40.693333333332845. Std of Reward: 45.83553885602486.\n",
      "Step: 167000. Mean Reward: 38.75666666666619. Std of Reward: 45.54256385209515.\n",
      "Step: 168000. Mean Reward: 42.35384615384571. Std of Reward: 43.85609536417558.\n",
      "Step: 169000. Mean Reward: 26.92040816326504. Std of Reward: 39.229065544654915.\n",
      "Step: 170000. Mean Reward: 15.901470588235307. Std of Reward: 23.61093024955037.\n",
      "Saved Model\n",
      "Step: 171000. Mean Reward: 62.9058823529405. Std of Reward: 40.850120175331796.\n",
      "Step: 172000. Mean Reward: 43.3655172413788. Std of Reward: 46.05010221907845.\n",
      "Step: 173000. Mean Reward: 21.94038461538461. Std of Reward: 29.822992289219282.\n",
      "Step: 174000. Mean Reward: 38.44999999999958. Std of Reward: 44.183868829085434.\n",
      "Step: 175000. Mean Reward: 45.57083333333287. Std of Reward: 44.18915288437695.\n",
      "Step: 176000. Mean Reward: 42.59999999999971. Std of Reward: 40.40332240717863.\n",
      "Step: 177000. Mean Reward: 8.867132867132852. Std of Reward: 19.063521993601352.\n",
      "Step: 178000. Mean Reward: 10.882191780821884. Std of Reward: 23.727865489241584.\n",
      "Step: 179000. Mean Reward: 32.08787878787852. Std of Reward: 40.551529191176535.\n",
      "Step: 180000. Mean Reward: 28.15599999999972. Std of Reward: 40.279737635689195.\n",
      "Saved Model\n",
      "Step: 181000. Mean Reward: 18.83392857142857. Std of Reward: 28.480559013284427.\n",
      "Step: 182000. Mean Reward: 38.79333333333313. Std of Reward: 39.7947478388232.\n",
      "Step: 183000. Mean Reward: 49.804347826086534. Std of Reward: 40.69110017276841.\n",
      "Step: 184000. Mean Reward: 31.1024999999998. Std of Reward: 36.11243198332076.\n",
      "Step: 185000. Mean Reward: 39.46499999999971. Std of Reward: 38.764213844730044.\n",
      "Step: 186000. Mean Reward: 30.915555555555237. Std of Reward: 41.634312534817575.\n",
      "Step: 187000. Mean Reward: 34.091666666666384. Std of Reward: 41.312154352226436.\n",
      "Step: 188000. Mean Reward: 37.12499999999973. Std of Reward: 39.41911021319437.\n",
      "Step: 189000. Mean Reward: 29.244736842105016. Std of Reward: 38.84664652171845.\n",
      "Step: 190000. Mean Reward: 45.946153846153365. Std of Reward: 44.08997121942952.\n",
      "Saved Model\n",
      "Step: 191000. Mean Reward: 29.62093023255785. Std of Reward: 38.723441360015165.\n",
      "Step: 192000. Mean Reward: 15.068965517241217. Std of Reward: 32.224434972154974.\n",
      "Step: 193000. Mean Reward: 26.98666666666642. Std of Reward: 38.84427370926068.\n",
      "Step: 194000. Mean Reward: 47.865384615384116. Std of Reward: 44.26118235331669.\n",
      "Step: 195000. Mean Reward: 44.283333333333054. Std of Reward: 40.387886660344705.\n",
      "Step: 196000. Mean Reward: 22.219672131147387. Std of Reward: 35.29817168753418.\n",
      "Step: 197000. Mean Reward: 16.562666666666612. Std of Reward: 27.895958002708515.\n",
      "Step: 198000. Mean Reward: 9.348888888888919. Std of Reward: 15.73223097205308.\n",
      "Step: 199000. Mean Reward: 5.916666666666686. Std of Reward: 10.219174515477222.\n",
      "Step: 200000. Mean Reward: 7.076870748299337. Std of Reward: 13.964698291443657.\n",
      "Saved Model\n",
      "Step: 201000. Mean Reward: 7.526811594202937. Std of Reward: 14.59072337736041.\n",
      "Step: 202000. Mean Reward: 9.439090909090906. Std of Reward: 16.578869878440997.\n",
      "Step: 203000. Mean Reward: 14.127692307692307. Std of Reward: 21.824104582511467.\n",
      "Step: 204000. Mean Reward: 24.416326530612086. Std of Reward: 34.05206905481312.\n",
      "Step: 205000. Mean Reward: 17.98805970149244. Std of Reward: 29.57745976391432.\n",
      "Step: 206000. Mean Reward: 15.149999999999933. Std of Reward: 27.549402992862557.\n",
      "Step: 207000. Mean Reward: 17.29122807017538. Std of Reward: 29.958622229005332.\n",
      "Step: 208000. Mean Reward: 20.459259259259117. Std of Reward: 33.48947053162104.\n",
      "Step: 209000. Mean Reward: 19.435483870967673. Std of Reward: 30.434101717423815.\n",
      "Step: 210000. Mean Reward: 26.405128205127994. Std of Reward: 38.20988560983746.\n",
      "Saved Model\n",
      "Step: 211000. Mean Reward: 36.90303030302991. Std of Reward: 43.28991403876747.\n",
      "Step: 212000. Mean Reward: 29.19999999999976. Std of Reward: 38.75667084365846.\n",
      "Step: 213000. Mean Reward: 12.078999999999935. Std of Reward: 24.334137728713266.\n",
      "Step: 214000. Mean Reward: 14.15657894736829. Std of Reward: 29.212048413401327.\n",
      "Step: 215000. Mean Reward: 21.313043478260656. Std of Reward: 35.453629290492124.\n",
      "Step: 216000. Mean Reward: 38.76896551724099. Std of Reward: 43.18953499879985.\n",
      "Step: 217000. Mean Reward: 28.67272727272706. Std of Reward: 38.07287488032467.\n",
      "Step: 218000. Mean Reward: 21.812244897959012. Std of Reward: 34.62514500170325.\n",
      "Step: 219000. Mean Reward: 21.401639344262136. Std of Reward: 34.96688124780031.\n",
      "Step: 220000. Mean Reward: 19.501960784313596. Std of Reward: 31.756089366318033.\n",
      "Saved Model\n",
      "Step: 221000. Mean Reward: 13.143157894736776. Std of Reward: 27.203241496601542.\n",
      "Step: 222000. Mean Reward: 6.937419354838696. Std of Reward: 16.13375203201627.\n",
      "Step: 223000. Mean Reward: 6.624203821656032. Std of Reward: 16.99839408403509.\n",
      "Step: 224000. Mean Reward: 8.009448818897608. Std of Reward: 19.89672719920287.\n",
      "Step: 225000. Mean Reward: 4.742325581395344. Std of Reward: 13.451457055547952.\n",
      "Step: 226000. Mean Reward: 4.016734693877542. Std of Reward: 12.262027229897722.\n",
      "Step: 227000. Mean Reward: 6.619259259259242. Std of Reward: 16.558178000932276.\n",
      "Step: 228000. Mean Reward: 8.605223880597029. Std of Reward: 16.099700178742452.\n",
      "Step: 229000. Mean Reward: 15.20178571428569. Std of Reward: 26.89323793839665.\n",
      "Step: 230000. Mean Reward: 17.176271186440538. Std of Reward: 32.07468688293461.\n",
      "Saved Model\n",
      "Step: 231000. Mean Reward: 30.547368421052475. Std of Reward: 36.46207645472968.\n",
      "Step: 232000. Mean Reward: 38.646874999999525. Std of Reward: 44.79437035202433.\n",
      "Step: 233000. Mean Reward: 43.86999999999954. Std of Reward: 45.07605535832315.\n",
      "Step: 234000. Mean Reward: 17.82153846153841. Std of Reward: 27.16194448861044.\n",
      "Step: 235000. Mean Reward: 18.866666666666603. Std of Reward: 27.65106946416475.\n",
      "Step: 236000. Mean Reward: 11.433846153846172. Std of Reward: 18.811659861521036.\n",
      "Step: 237000. Mean Reward: 44.92499999999952. Std of Reward: 45.54559430630183.\n",
      "Step: 238000. Mean Reward: 31.325714285714092. Std of Reward: 37.28115834234377.\n",
      "Step: 239000. Mean Reward: 53.226923076922795. Std of Reward: 37.182725080051114.\n",
      "Step: 240000. Mean Reward: 33.456249999999834. Std of Reward: 38.12509784823474.\n",
      "Saved Model\n",
      "Step: 241000. Mean Reward: 31.002631578947103. Std of Reward: 39.62911932326922.\n",
      "Step: 242000. Mean Reward: 40.4119999999996. Std of Reward: 43.254764546809724.\n",
      "Step: 243000. Mean Reward: 58.26842105263084. Std of Reward: 44.362668666245426.\n",
      "Step: 244000. Mean Reward: 40.344827586206456. Std of Reward: 45.01103762110844.\n",
      "Step: 245000. Mean Reward: 39.99696969696926. Std of Reward: 44.66780743578356.\n",
      "Step: 246000. Mean Reward: 55.38636363636295. Std of Reward: 45.77770691519137.\n",
      "Step: 247000. Mean Reward: 45.35599999999944. Std of Reward: 46.554991826870086.\n",
      "Step: 248000. Mean Reward: 33.40555555555516. Std of Reward: 43.93273864319529.\n",
      "Step: 249000. Mean Reward: 39.68846153846112. Std of Reward: 45.12586949448267.\n",
      "Step: 250000. Mean Reward: 20.80666666666652. Std of Reward: 33.96575661194957.\n",
      "Saved Model\n",
      "Step: 251000. Mean Reward: 24.07872340425514. Std of Reward: 36.9127605000219.\n",
      "Step: 252000. Mean Reward: 55.27083333333271. Std of Reward: 45.79623782916562.\n",
      "Step: 253000. Mean Reward: 61.41999999999931. Std of Reward: 44.7362783431964.\n",
      "Step: 254000. Mean Reward: 40.551724137930606. Std of Reward: 44.47055298997517.\n",
      "Step: 255000. Mean Reward: 42.042307692307375. Std of Reward: 41.79702945901287.\n",
      "Step: 256000. Mean Reward: 11.450833333333321. Std of Reward: 22.47722920288183.\n",
      "Step: 257000. Mean Reward: 10.153684210526304. Std of Reward: 22.373117376964473.\n",
      "Step: 258000. Mean Reward: 16.972549019607804. Std of Reward: 27.908380064350112.\n",
      "Step: 259000. Mean Reward: 32.05365853658507. Std of Reward: 41.396842683664225.\n",
      "Step: 260000. Mean Reward: 26.699999999999843. Std of Reward: 36.40491549695448.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Step: 261000. Mean Reward: 11.936363636363621. Std of Reward: 20.035632170556152.\n",
      "Step: 262000. Mean Reward: 22.484482758620587. Std of Reward: 32.60539833516084.\n",
      "Step: 263000. Mean Reward: 23.202173913043364. Std of Reward: 34.30360919742604.\n",
      "Step: 264000. Mean Reward: 25.01730769230752. Std of Reward: 36.77278321067283.\n",
      "Step: 265000. Mean Reward: 24.697560975609615. Std of Reward: 36.060352444774814.\n",
      "Step: 266000. Mean Reward: 16.655813953488273. Std of Reward: 30.87792394459443.\n",
      "Step: 267000. Mean Reward: 4.100881057268728. Std of Reward: 10.40499706782501.\n",
      "Step: 268000. Mean Reward: 2.995238095238097. Std of Reward: 8.438326963305439.\n",
      "Step: 269000. Mean Reward: 3.6019157088122564. Std of Reward: 9.302858507291564.\n",
      "Step: 270000. Mean Reward: 3.4190311418685257. Std of Reward: 9.437106568021473.\n",
      "Saved Model\n",
      "Step: 271000. Mean Reward: 1.0440000000000003. Std of Reward: 1.8040872114913593.\n",
      "Step: 272000. Mean Reward: 1.332112068965519. Std of Reward: 3.419988004797375.\n",
      "Step: 273000. Mean Reward: 1.0573236889692599. Std of Reward: 2.866055262032929.\n",
      "Step: 274000. Mean Reward: 0.9391224862888488. Std of Reward: 1.702585827119523.\n",
      "Step: 275000. Mean Reward: 2.145977011494253. Std of Reward: 4.060909942696339.\n",
      "Step: 276000. Mean Reward: 3.023297491039433. Std of Reward: 6.555125649495612.\n",
      "Step: 277000. Mean Reward: 2.518879056047197. Std of Reward: 3.9262520029517134.\n",
      "Step: 278000. Mean Reward: 3.4780701754385985. Std of Reward: 5.809267531990036.\n",
      "Step: 279000. Mean Reward: 5.466304347826098. Std of Reward: 9.373236381058554.\n",
      "Step: 280000. Mean Reward: 9.092857142857163. Std of Reward: 15.902663641175113.\n",
      "Saved Model\n",
      "Step: 281000. Mean Reward: 10.702727272727238. Std of Reward: 22.293229543318002.\n",
      "Step: 282000. Mean Reward: 14.417283950617255. Std of Reward: 25.170572618762563.\n",
      "Step: 283000. Mean Reward: 9.178333333333326. Std of Reward: 19.853245676434994.\n",
      "Step: 284000. Mean Reward: 8.83589743589741. Std of Reward: 19.468503871587583.\n",
      "Step: 285000. Mean Reward: 6.641397849462361. Std of Reward: 15.450620483166114.\n",
      "Step: 286000. Mean Reward: 5.961029411764718. Std of Reward: 12.26355895945696.\n",
      "Step: 287000. Mean Reward: 6.391875000000017. Std of Reward: 13.899941420177855.\n",
      "Step: 288000. Mean Reward: 9.72680412371132. Std of Reward: 20.34863918276831.\n",
      "Step: 289000. Mean Reward: 12.408823529411734. Std of Reward: 23.923305164388506.\n",
      "Step: 290000. Mean Reward: 6.7283018867924715. Std of Reward: 13.246208106166996.\n",
      "Saved Model\n",
      "Step: 291000. Mean Reward: 7.564963503649634. Std of Reward: 14.977889257318811.\n",
      "Step: 292000. Mean Reward: 8.86153846153847. Std of Reward: 18.896628248753686.\n",
      "Step: 293000. Mean Reward: 6.247590361445769. Std of Reward: 15.936896274630671.\n",
      "Step: 294000. Mean Reward: 5.705027932960903. Std of Reward: 12.888842664207033.\n",
      "Step: 295000. Mean Reward: 8.404958677685983. Std of Reward: 16.573736324139333.\n",
      "Step: 296000. Mean Reward: 7.007534246575345. Std of Reward: 14.92894625838717.\n",
      "Step: 297000. Mean Reward: 8.417647058823478. Std of Reward: 21.505966621986804.\n",
      "Step: 298000. Mean Reward: 7.1532374100719505. Std of Reward: 16.62290307934697.\n",
      "Step: 299000. Mean Reward: 3.6137037037037048. Std of Reward: 9.22460865300512.\n",
      "Step: 300000. Mean Reward: 4.281818181818198. Std of Reward: 9.516346527931336.\n",
      "Saved Model\n",
      "Step: 301000. Mean Reward: 4.284162895927604. Std of Reward: 10.642005596821765.\n",
      "Step: 302000. Mean Reward: 2.7128205128205174. Std of Reward: 5.4746234848725495.\n",
      "Step: 303000. Mean Reward: 2.655017301038066. Std of Reward: 5.291435668671325.\n",
      "Step: 304000. Mean Reward: 3.661977186311795. Std of Reward: 7.944201743276877.\n",
      "Step: 305000. Mean Reward: 3.8354430379746844. Std of Reward: 7.682750221376109.\n",
      "Step: 306000. Mean Reward: 4.778461538461544. Std of Reward: 10.948829591696443.\n",
      "Step: 307000. Mean Reward: 2.9689873417721504. Std of Reward: 7.181083641461216.\n",
      "Step: 308000. Mean Reward: 1.694146341463415. Std of Reward: 1.9714708828686274.\n",
      "Step: 309000. Mean Reward: 1.8690721649484554. Std of Reward: 3.3642560435780022.\n",
      "Step: 310000. Mean Reward: 2.0229333333333313. Std of Reward: 4.762504319741379.\n",
      "Saved Model\n",
      "Step: 311000. Mean Reward: 2.5996688741721865. Std of Reward: 5.66037404483501.\n",
      "Step: 312000. Mean Reward: 3.3521235521235577. Std of Reward: 6.887004810428606.\n",
      "Step: 313000. Mean Reward: 3.338043478260866. Std of Reward: 8.196325295273178.\n",
      "Step: 314000. Mean Reward: 2.758075601374573. Std of Reward: 4.5565286880482025.\n",
      "Step: 315000. Mean Reward: 3.5684410646387903. Std of Reward: 6.154941154190352.\n",
      "Step: 316000. Mean Reward: 3.370886075949375. Std of Reward: 7.806139830037231.\n",
      "Step: 317000. Mean Reward: 4.6694444444444505. Std of Reward: 10.790706377981126.\n",
      "Step: 318000. Mean Reward: 3.3438297872340508. Std of Reward: 7.639527264307135.\n",
      "Step: 319000. Mean Reward: 4.405454545454549. Std of Reward: 10.239303388624498.\n",
      "Step: 320000. Mean Reward: 3.731835205992502. Std of Reward: 9.830431121408983.\n",
      "Saved Model\n",
      "Step: 321000. Mean Reward: 2.5901273885350324. Std of Reward: 3.986745314204653.\n",
      "Step: 322000. Mean Reward: 2.0110552763819096. Std of Reward: 3.0541229061037765.\n",
      "Step: 323000. Mean Reward: 1.8787012987012985. Std of Reward: 2.2875753344485816.\n",
      "Step: 324000. Mean Reward: 1.771712158808933. Std of Reward: 2.0863478714540036.\n",
      "Step: 325000. Mean Reward: 1.8202046035805621. Std of Reward: 2.079697489087512.\n",
      "Step: 326000. Mean Reward: 2.1598337950138498. Std of Reward: 5.041794186104297.\n",
      "Step: 327000. Mean Reward: 1.9907651715039572. Std of Reward: 2.6880943795567123.\n",
      "Step: 328000. Mean Reward: 1.8613810741687975. Std of Reward: 1.96655335752158.\n",
      "Step: 329000. Mean Reward: 1.6739336492890988. Std of Reward: 1.9259719465053307.\n",
      "Step: 330000. Mean Reward: 1.4829596412556054. Std of Reward: 1.4650873443662962.\n",
      "Saved Model\n",
      "Step: 331000. Mean Reward: 1.5389521640091117. Std of Reward: 1.5119899829092327.\n",
      "Step: 332000. Mean Reward: 1.3917570498915401. Std of Reward: 1.3803931320858307.\n",
      "Step: 333000. Mean Reward: 1.567816091954023. Std of Reward: 1.606874187929159.\n",
      "Step: 334000. Mean Reward: 1.4228070175438596. Std of Reward: 1.376878945087197.\n",
      "Step: 335000. Mean Reward: 1.798997493734336. Std of Reward: 1.605800574856432.\n",
      "Step: 336000. Mean Reward: 1.7987562189054724. Std of Reward: 1.5642216907410216.\n",
      "Step: 337000. Mean Reward: 1.8335877862595416. Std of Reward: 1.8477526692639172.\n",
      "Step: 338000. Mean Reward: 2.034316353887399. Std of Reward: 1.9345044953547015.\n",
      "Step: 339000. Mean Reward: 2.2577586206896543. Std of Reward: 2.0982676569319785.\n",
      "Step: 340000. Mean Reward: 2.387951807228915. Std of Reward: 2.3087522103425857.\n",
      "Saved Model\n",
      "Step: 341000. Mean Reward: 2.382089552238806. Std of Reward: 2.699680764606511.\n",
      "Step: 342000. Mean Reward: 2.5859872611464962. Std of Reward: 3.094426026415681.\n",
      "Step: 343000. Mean Reward: 2.7174193548387096. Std of Reward: 3.6579678544434433.\n",
      "Step: 344000. Mean Reward: 2.9562500000000003. Std of Reward: 5.219504270176341.\n",
      "Step: 345000. Mean Reward: 2.852189781021899. Std of Reward: 3.8688460369228874.\n",
      "Step: 346000. Mean Reward: 3.9526086956521747. Std of Reward: 9.266642709619806.\n",
      "Step: 347000. Mean Reward: 5.38388888888891. Std of Reward: 10.642650901437968.\n",
      "Step: 348000. Mean Reward: 6.849166666666688. Std of Reward: 12.171626814257715.\n",
      "Step: 349000. Mean Reward: 10.378181818181831. Std of Reward: 20.782415564467314.\n",
      "Step: 350000. Mean Reward: 6.772368421052638. Std of Reward: 14.741443536793673.\n",
      "Saved Model\n",
      "Step: 351000. Mean Reward: 10.440707964601806. Std of Reward: 20.00516144535055.\n",
      "Step: 352000. Mean Reward: 7.104511278195478. Std of Reward: 16.766762971743276.\n",
      "Step: 353000. Mean Reward: 5.977653631284927. Std of Reward: 13.88362457595504.\n",
      "Step: 354000. Mean Reward: 7.693793103448306. Std of Reward: 16.46029379504706.\n",
      "Step: 355000. Mean Reward: 6.406206896551737. Std of Reward: 14.140434664780239.\n",
      "Step: 356000. Mean Reward: 5.044392523364499. Std of Reward: 12.117849711905093.\n",
      "Step: 357000. Mean Reward: 1.748514851485151. Std of Reward: 4.187744114034558.\n",
      "Step: 358000. Mean Reward: 1.6831018518518521. Std of Reward: 3.258800990707113.\n",
      "Step: 359000. Mean Reward: 2.2491228070175477. Std of Reward: 4.984512486453291.\n",
      "Step: 360000. Mean Reward: 2.633564013840836. Std of Reward: 5.841944866545505.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Step: 361000. Mean Reward: 3.547407407407416. Std of Reward: 8.00758741703313.\n",
      "Step: 362000. Mean Reward: 3.3972972972973015. Std of Reward: 5.934471026414412.\n",
      "Step: 363000. Mean Reward: 2.3773913043478268. Std of Reward: 3.4494459341578456.\n",
      "Step: 364000. Mean Reward: 2.573559322033899. Std of Reward: 4.228416285133162.\n",
      "Step: 365000. Mean Reward: 4.3981566820276505. Std of Reward: 9.517037659837651.\n",
      "Step: 366000. Mean Reward: 7.6301587301587634. Std of Reward: 13.156387004889538.\n",
      "Step: 367000. Mean Reward: 5.635359116022091. Std of Reward: 14.650569672264012.\n",
      "Step: 368000. Mean Reward: 7.072932330827062. Std of Reward: 15.048019017925238.\n",
      "Step: 369000. Mean Reward: 8.077586206896546. Std of Reward: 18.505227994788857.\n",
      "Step: 370000. Mean Reward: 19.448148148148043. Std of Reward: 31.304151545410605.\n",
      "Saved Model\n",
      "Step: 371000. Mean Reward: 11.924778761061916. Std of Reward: 23.96654944308175.\n",
      "Step: 372000. Mean Reward: 9.703960396039607. Std of Reward: 19.0482083603945.\n",
      "Step: 373000. Mean Reward: 7.884353741496614. Std of Reward: 15.709716654716145.\n",
      "Step: 374000. Mean Reward: 5.460000000000001. Std of Reward: 12.131028327221973.\n",
      "Step: 375000. Mean Reward: 9.78943089430894. Std of Reward: 20.12957946890136.\n",
      "Step: 376000. Mean Reward: 7.465034965034965. Std of Reward: 15.418349157190645.\n",
      "Step: 377000. Mean Reward: 10.316853932584275. Std of Reward: 19.272558566463427.\n",
      "Step: 378000. Mean Reward: 12.194791666666623. Std of Reward: 24.71747176337527.\n",
      "Step: 379000. Mean Reward: 12.04999999999992. Std of Reward: 26.013890520258297.\n",
      "Step: 380000. Mean Reward: 6.400497512437787. Std of Reward: 16.82091063463411.\n",
      "Saved Model\n",
      "Step: 381000. Mean Reward: 5.641489361702115. Std of Reward: 14.19490775920848.\n",
      "Step: 382000. Mean Reward: 5.063281249999997. Std of Reward: 10.470985739575676.\n",
      "Step: 383000. Mean Reward: 7.240697674418557. Std of Reward: 20.748484830682333.\n",
      "Step: 384000. Mean Reward: 6.184615384615359. Std of Reward: 17.987559803717204.\n",
      "Step: 385000. Mean Reward: 6.22049689440994. Std of Reward: 15.784956743064702.\n",
      "Step: 386000. Mean Reward: 3.8799256505576345. Std of Reward: 9.364099680148229.\n",
      "Step: 387000. Mean Reward: 3.1120689655172487. Std of Reward: 7.951626977006104.\n",
      "Step: 388000. Mean Reward: 1.4216157205240172. Std of Reward: 1.9932944380994233.\n",
      "Step: 389000. Mean Reward: 1.3330396475770927. Std of Reward: 1.8528014301044866.\n",
      "Step: 390000. Mean Reward: 1.9213930348258723. Std of Reward: 3.9772912667603455.\n",
      "Saved Model\n",
      "Step: 391000. Mean Reward: 1.9182890855457226. Std of Reward: 2.6692759769108423.\n",
      "Step: 392000. Mean Reward: 6.002339181286556. Std of Reward: 12.324847498985935.\n",
      "Step: 393000. Mean Reward: 5.71161290322581. Std of Reward: 9.923572164060559.\n",
      "Step: 394000. Mean Reward: 10.556140350877214. Std of Reward: 20.305504547478503.\n",
      "Step: 395000. Mean Reward: 5.924444444444455. Std of Reward: 11.744590349141031.\n",
      "Step: 396000. Mean Reward: 5.54451219512196. Std of Reward: 9.494319566474452.\n",
      "Step: 397000. Mean Reward: 8.270967741935515. Std of Reward: 15.778189183242421.\n",
      "Step: 398000. Mean Reward: 3.659176029962556. Std of Reward: 7.428396023763689.\n",
      "Step: 399000. Mean Reward: 3.110943396226418. Std of Reward: 5.150181628653203.\n",
      "Step: 400000. Mean Reward: 3.989473684210525. Std of Reward: 8.761197030497685.\n",
      "Saved Model\n",
      "Step: 401000. Mean Reward: 3.874358974358973. Std of Reward: 8.842377120214563.\n",
      "Step: 402000. Mean Reward: 4.538020833333345. Std of Reward: 9.456576318426965.\n",
      "Step: 403000. Mean Reward: 3.794716981132081. Std of Reward: 9.077002373565406.\n",
      "Step: 404000. Mean Reward: 3.613584905660385. Std of Reward: 7.620253819954279.\n",
      "Step: 405000. Mean Reward: 2.588025889967639. Std of Reward: 4.226203394539349.\n",
      "Step: 406000. Mean Reward: 4.3353535353535495. Std of Reward: 9.26646297063368.\n",
      "Step: 407000. Mean Reward: 5.375510204081643. Std of Reward: 12.41781604598559.\n",
      "Step: 408000. Mean Reward: 3.086986301369868. Std of Reward: 5.914450190390245.\n",
      "Step: 409000. Mean Reward: 2.9645390070922035. Std of Reward: 5.794123150689691.\n",
      "Step: 410000. Mean Reward: 3.3271653543307145. Std of Reward: 6.294886863653836.\n",
      "Saved Model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-4774e10c7fc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'actions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbuffer_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;31m# Perform gradient descent with experience buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msummary_freq\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;31m# Write training statistics to tensorboard.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\pj5\\ml-agents\\python\\ppo\\trainer.py\u001b[0m in \u001b[0;36mupdate_model\u001b[1;34m(self, batch_size, num_epoch)\u001b[0m\n\u001b[0;32m    183\u001b[0m                         \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_in\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'observations%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                 v_loss, p_loss, _ = self.sess.run([self.model.value_loss, self.model.policy_loss,\n\u001b[1;32m--> 185\u001b[1;33m                                                    self.model.update_batch], feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m    186\u001b[0m                 \u001b[0mtotal_v\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mv_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mtotal_p\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mp_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "\n",
    "with tf.Session(config = config) as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "            #export_graph(model_path, env_name+\"_\"+str(steps))\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/Ball3\\model-160000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/Ball3\\model-160000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 7 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 7 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 7 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
