{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 10000000 # Set maximum number of steps to run environment.\n",
    "run_path = \"crawler\" # The sub-directory name for model and summary statistics\n",
    "load_model = True #Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 10000 # Frequency at which to save training statistics.\n",
    "save_freq = 50000 # Frequency at which to save model.\n",
    "env_name = \"Crawler/Crawler\" # Name of the training environment file.\n",
    "curriculum_file = None\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 2 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 2048 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 3e-4 # Model learning rate.\n",
    "hidden_units = 64 # Number of units in hidden layer.\n",
    "batch_size = 64 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\tsteps -> 0.0\n",
      "Unity brain name: Brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 117\n",
      "        Action space type: continuous\n",
      "        Action space size (per agent): 12\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: , , , , , , , , , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./models/crawler\\model-1350000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/crawler\\model-1350000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Step: 1360000. Mean Reward: 2.468955835638274. Std of Reward: 9.781628110894262.\n",
      "Step: 1370000. Mean Reward: 1.4340189211469656. Std of Reward: 11.777048111938557.\n",
      "Step: 1380000. Mean Reward: 5.2115113053274635. Std of Reward: 11.01062740808398.\n",
      "Step: 1390000. Mean Reward: 7.845564764916154. Std of Reward: 10.267377768200733.\n",
      "Step: 1400000. Mean Reward: 6.399193492423767. Std of Reward: 10.123383969432219.\n",
      "Saved Model\n",
      "Step: 1410000. Mean Reward: 5.373019442050786. Std of Reward: 11.61908736471304.\n",
      "Step: 1420000. Mean Reward: 8.138599096497519. Std of Reward: 9.843359509574052.\n",
      "Step: 1430000. Mean Reward: 9.127924831643412. Std of Reward: 9.300962869296274.\n",
      "Step: 1440000. Mean Reward: 8.895197389702439. Std of Reward: 10.210636996247466.\n",
      "Step: 1450000. Mean Reward: 8.180352135393694. Std of Reward: 8.249521480811467.\n",
      "Saved Model\n",
      "Step: 1460000. Mean Reward: 3.407236283811995. Std of Reward: 12.069017133054533.\n",
      "Step: 1470000. Mean Reward: 5.687741912136717. Std of Reward: 9.267072569900009.\n",
      "Step: 1480000. Mean Reward: 7.4573546974758615. Std of Reward: 9.465672955590327.\n",
      "Step: 1490000. Mean Reward: 7.898902217459776. Std of Reward: 10.669867587849758.\n",
      "Step: 1500000. Mean Reward: 6.788553613011622. Std of Reward: 8.640253870553462.\n",
      "Saved Model\n",
      "Step: 1510000. Mean Reward: 4.198377138908852. Std of Reward: 14.576990071468602.\n",
      "Step: 1520000. Mean Reward: -0.8712586009682532. Std of Reward: 12.453550087186027.\n",
      "Step: 1530000. Mean Reward: 3.9406780486811566. Std of Reward: 8.056604925136293.\n",
      "Step: 1540000. Mean Reward: 3.4507253627827614. Std of Reward: 12.010504044804204.\n",
      "Step: 1550000. Mean Reward: 4.293056880534306. Std of Reward: 8.311474281119988.\n",
      "Saved Model\n",
      "Step: 1560000. Mean Reward: 2.22712468969699. Std of Reward: 9.16700304045788.\n",
      "Step: 1570000. Mean Reward: 3.6393937833746044. Std of Reward: 7.810926558021175.\n",
      "Step: 1580000. Mean Reward: 3.533962324748356. Std of Reward: 9.982872780429256.\n",
      "Step: 1590000. Mean Reward: 4.594390214416533. Std of Reward: 6.399222612784506.\n",
      "Step: 1600000. Mean Reward: 3.039657470200453. Std of Reward: 4.856139914619855.\n",
      "Saved Model\n",
      "Step: 1610000. Mean Reward: 7.160616013121563. Std of Reward: 6.366810814985919.\n",
      "Step: 1620000. Mean Reward: 5.293938704737453. Std of Reward: 6.591455701360352.\n",
      "Step: 1630000. Mean Reward: 6.564491666048024. Std of Reward: 8.029087012096715.\n",
      "Step: 1640000. Mean Reward: 3.3090352271074703. Std of Reward: 9.378404245686362.\n",
      "Step: 1650000. Mean Reward: 1.7328888555444262. Std of Reward: 10.603738352998944.\n",
      "Saved Model\n",
      "Step: 1660000. Mean Reward: 2.259816242880428. Std of Reward: 7.7382305999166014.\n",
      "Step: 1670000. Mean Reward: 4.467909129675252. Std of Reward: 6.843026905007974.\n",
      "Step: 1680000. Mean Reward: 4.327095472248087. Std of Reward: 8.608860012412913.\n",
      "Step: 1690000. Mean Reward: 2.4929034708525095. Std of Reward: 11.388806908999427.\n",
      "Step: 1700000. Mean Reward: 4.699455746145823. Std of Reward: 8.240226348634522.\n",
      "Saved Model\n",
      "Step: 1710000. Mean Reward: 7.509255033355484. Std of Reward: 7.681856876566815.\n",
      "Step: 1720000. Mean Reward: 4.101900486347751. Std of Reward: 15.4936762546693.\n",
      "Step: 1730000. Mean Reward: 1.09177661067798. Std of Reward: 15.311136426999873.\n",
      "Step: 1740000. Mean Reward: 3.500301593779422. Std of Reward: 8.340257654296137.\n",
      "Step: 1750000. Mean Reward: 3.128390982637728. Std of Reward: 6.602073902102898.\n",
      "Saved Model\n",
      "Step: 1760000. Mean Reward: 3.9713423549541753. Std of Reward: 6.37036359481746.\n",
      "Step: 1770000. Mean Reward: 3.568421503711227. Std of Reward: 6.524253257787507.\n",
      "Step: 1780000. Mean Reward: 4.73713369625974. Std of Reward: 5.685532256109679.\n",
      "Step: 1790000. Mean Reward: 3.865160211950909. Std of Reward: 8.418354781276948.\n",
      "Step: 1800000. Mean Reward: 4.5871772735881. Std of Reward: 12.531115656619834.\n",
      "Saved Model\n",
      "Step: 1810000. Mean Reward: 4.112326387285613. Std of Reward: 11.674828677777244.\n",
      "Step: 1820000. Mean Reward: 0.20614778140847584. Std of Reward: 11.73672260051952.\n",
      "Step: 1830000. Mean Reward: 2.893892037744558. Std of Reward: 10.934658617734783.\n",
      "Step: 1840000. Mean Reward: 5.34111830255551. Std of Reward: 8.195808029427612.\n",
      "Step: 1850000. Mean Reward: 5.8802159096722875. Std of Reward: 8.007282196420839.\n",
      "Saved Model\n",
      "Step: 1860000. Mean Reward: 5.752500395156096. Std of Reward: 7.649661129504591.\n",
      "Step: 1870000. Mean Reward: 5.002771527891638. Std of Reward: 8.51328871148903.\n",
      "Step: 1880000. Mean Reward: 6.851974934862876. Std of Reward: 8.804869782053983.\n",
      "Step: 1890000. Mean Reward: 5.860011863836133. Std of Reward: 11.022139382612371.\n",
      "Step: 1900000. Mean Reward: 7.068979107998362. Std of Reward: 10.02155835238296.\n",
      "Saved Model\n",
      "Step: 1910000. Mean Reward: 7.163481069675263. Std of Reward: 8.887621957668257.\n",
      "Step: 1920000. Mean Reward: 6.229924974270299. Std of Reward: 8.50278469906413.\n",
      "Step: 1930000. Mean Reward: 5.259095375930271. Std of Reward: 11.018077695909332.\n",
      "Step: 1940000. Mean Reward: 5.7779228396271405. Std of Reward: 12.175506846920731.\n",
      "Step: 1950000. Mean Reward: 6.627697281181843. Std of Reward: 11.18526989940305.\n",
      "Saved Model\n",
      "Step: 1960000. Mean Reward: 4.655286648379202. Std of Reward: 14.797293293343357.\n",
      "Step: 1970000. Mean Reward: 3.6536206937649225. Std of Reward: 13.153017112590032.\n",
      "Step: 1980000. Mean Reward: 4.0121974823888396. Std of Reward: 9.885375668054596.\n",
      "Step: 1990000. Mean Reward: 4.933261393748806. Std of Reward: 9.231795981314685.\n",
      "Step: 2000000. Mean Reward: 4.10064458292338. Std of Reward: 10.211393458144913.\n",
      "Saved Model\n",
      "Step: 2010000. Mean Reward: 5.279077262973927. Std of Reward: 10.926341981809568.\n",
      "Step: 2020000. Mean Reward: 5.514055122479193. Std of Reward: 9.594427294526302.\n",
      "Step: 2030000. Mean Reward: 4.012148877116689. Std of Reward: 11.43499377363681.\n",
      "Step: 2040000. Mean Reward: 5.247335060864221. Std of Reward: 10.309842393715673.\n",
      "Step: 2050000. Mean Reward: 3.575143276085424. Std of Reward: 11.892222310421566.\n",
      "Saved Model\n",
      "Step: 2060000. Mean Reward: 2.7746309848794763. Std of Reward: 11.176550035904619.\n",
      "Step: 2070000. Mean Reward: 3.768182930435398. Std of Reward: 10.285314886415987.\n",
      "Step: 2080000. Mean Reward: 4.467014282200854. Std of Reward: 10.966950771396549.\n",
      "Step: 2090000. Mean Reward: 5.725715258339516. Std of Reward: 9.82075348509687.\n",
      "Step: 2100000. Mean Reward: 5.66758022541042. Std of Reward: 11.057433977915045.\n",
      "Saved Model\n",
      "Step: 2110000. Mean Reward: 2.690728359126566. Std of Reward: 13.35766499928699.\n",
      "Step: 2120000. Mean Reward: 0.5010950545221682. Std of Reward: 13.792530927454806.\n",
      "Step: 2130000. Mean Reward: 2.977872209459128. Std of Reward: 11.132681504929781.\n",
      "Step: 2140000. Mean Reward: 0.6973680934674964. Std of Reward: 13.615987214196311.\n",
      "Step: 2150000. Mean Reward: 2.596162443612711. Std of Reward: 10.085382218728139.\n",
      "Saved Model\n",
      "Step: 2160000. Mean Reward: 2.5625702162353114. Std of Reward: 8.760836064101218.\n",
      "Step: 2170000. Mean Reward: 1.971770111534012. Std of Reward: 9.115394527718466.\n",
      "Step: 2180000. Mean Reward: 2.3224349504639936. Std of Reward: 10.201072467467995.\n",
      "Step: 2190000. Mean Reward: 3.242723321303305. Std of Reward: 8.217084068146649.\n",
      "Step: 2200000. Mean Reward: 3.7105532369109513. Std of Reward: 7.5968352761423334.\n",
      "Saved Model\n",
      "Step: 2210000. Mean Reward: 4.242061410611151. Std of Reward: 7.260846005474082.\n",
      "Step: 2220000. Mean Reward: 4.0998869136436475. Std of Reward: 6.014526513869813.\n",
      "Step: 2230000. Mean Reward: 3.5896758232025405. Std of Reward: 6.895363663472726.\n",
      "Step: 2240000. Mean Reward: 3.3289376452722785. Std of Reward: 6.364207892724369.\n",
      "Step: 2250000. Mean Reward: 4.665750282216376. Std of Reward: 7.214313907019857.\n",
      "Saved Model\n",
      "Step: 2260000. Mean Reward: 5.1987823432539155. Std of Reward: 7.001081421243587.\n",
      "Step: 2270000. Mean Reward: 4.019017122793698. Std of Reward: 8.387757075790676.\n",
      "Step: 2280000. Mean Reward: 3.858166498170491. Std of Reward: 7.793503640480706.\n",
      "Step: 2290000. Mean Reward: 5.04995948287188. Std of Reward: 7.7533030364859465.\n",
      "Step: 2300000. Mean Reward: 5.634366553566112. Std of Reward: 7.537660239799231.\n",
      "Saved Model\n",
      "Step: 2310000. Mean Reward: 5.220416510349615. Std of Reward: 5.405815703699847.\n",
      "Step: 2320000. Mean Reward: 6.7082107860962825. Std of Reward: 4.76552809712855.\n",
      "Step: 2330000. Mean Reward: 6.827216508941692. Std of Reward: 5.6918022937738035.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2340000. Mean Reward: 6.618874971127353. Std of Reward: 7.036172665652465.\n",
      "Step: 2350000. Mean Reward: 7.101808625085161. Std of Reward: 6.03034788180047.\n",
      "Saved Model\n",
      "Step: 2360000. Mean Reward: 6.154944682990578. Std of Reward: 6.0591056340826945.\n",
      "Step: 2370000. Mean Reward: 6.129918882327586. Std of Reward: 6.153445954243045.\n",
      "Step: 2380000. Mean Reward: 6.306931706622007. Std of Reward: 5.62047710304063.\n",
      "Step: 2390000. Mean Reward: 5.987782854509906. Std of Reward: 6.310515651040099.\n",
      "Step: 2400000. Mean Reward: 5.281214656908662. Std of Reward: 6.475285407602928.\n",
      "Saved Model\n",
      "Step: 2410000. Mean Reward: 3.5939251624930706. Std of Reward: 6.510365932364829.\n",
      "Step: 2420000. Mean Reward: 2.877269064698895. Std of Reward: 6.26516128341908.\n",
      "Step: 2430000. Mean Reward: 2.257775905253586. Std of Reward: 7.4002756162692.\n",
      "Step: 2440000. Mean Reward: 3.6529210931966074. Std of Reward: 7.791300604470388.\n",
      "Step: 2450000. Mean Reward: 3.3743878709992132. Std of Reward: 7.346455043365553.\n",
      "Saved Model\n",
      "Step: 2460000. Mean Reward: 1.3785141212707874. Std of Reward: 14.094771750896692.\n",
      "Step: 2470000. Mean Reward: 4.910339178958897. Std of Reward: 10.298561350053065.\n",
      "Step: 2480000. Mean Reward: 7.604962375909549. Std of Reward: 5.903973886720135.\n",
      "Step: 2490000. Mean Reward: 5.62078514971191. Std of Reward: 5.593965894890998.\n",
      "Step: 2500000. Mean Reward: 2.6985506155691232. Std of Reward: 6.545816988807006.\n",
      "Saved Model\n",
      "Step: 2510000. Mean Reward: 3.072231088468748. Std of Reward: 5.869964992995349.\n",
      "Step: 2520000. Mean Reward: 3.468105212135895. Std of Reward: 6.810284771406323.\n",
      "Step: 2530000. Mean Reward: 4.997352148376379. Std of Reward: 7.381018419089385.\n",
      "Step: 2540000. Mean Reward: 5.516410329165754. Std of Reward: 7.915190191946861.\n",
      "Step: 2550000. Mean Reward: 4.676050781091292. Std of Reward: 6.71713051308661.\n",
      "Saved Model\n",
      "Step: 2560000. Mean Reward: 3.893870749265793. Std of Reward: 8.169796005288713.\n",
      "Step: 2570000. Mean Reward: 4.340438309268736. Std of Reward: 6.9826925906938575.\n",
      "Step: 2580000. Mean Reward: 5.431322641423459. Std of Reward: 5.10579434445405.\n",
      "Step: 2590000. Mean Reward: 4.100213312278629. Std of Reward: 7.469982825679871.\n",
      "Step: 2600000. Mean Reward: 4.224358140789979. Std of Reward: 7.785009081965734.\n",
      "Saved Model\n",
      "Step: 2610000. Mean Reward: 2.6301862257739526. Std of Reward: 10.469314617384487.\n",
      "Step: 2620000. Mean Reward: 0.04187520899289995. Std of Reward: 16.70561685195299.\n",
      "Step: 2630000. Mean Reward: 1.664625873160865. Std of Reward: 6.004949794231925.\n",
      "Step: 2640000. Mean Reward: 2.9044489715804525. Std of Reward: 6.159637249533291.\n",
      "Step: 2650000. Mean Reward: 3.6721874913365844. Std of Reward: 6.638898619047223.\n",
      "Saved Model\n",
      "Step: 2660000. Mean Reward: 3.7989760354114215. Std of Reward: 6.4711055328643505.\n",
      "Step: 2670000. Mean Reward: 3.327827058037279. Std of Reward: 6.921935221773206.\n",
      "Step: 2680000. Mean Reward: 3.4397457678971373. Std of Reward: 9.871895623592735.\n",
      "Step: 2690000. Mean Reward: 4.901806449963483. Std of Reward: 8.637856918209414.\n",
      "Step: 2700000. Mean Reward: 5.900160018449409. Std of Reward: 8.441761207128991.\n",
      "Saved Model\n",
      "Step: 2710000. Mean Reward: 4.948816471602309. Std of Reward: 9.503217089183348.\n",
      "Step: 2720000. Mean Reward: 5.097543249867075. Std of Reward: 8.530482103134968.\n",
      "Step: 2730000. Mean Reward: 4.025161423412779. Std of Reward: 6.005599344198999.\n",
      "Step: 2740000. Mean Reward: 3.355942935321642. Std of Reward: 5.590770020097889.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-78e447cd20e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# Decide and take an action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mnew_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_experiences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_horizon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\pj5\\ml-agents\\python\\ppo\\trainer.py\u001b[0m in \u001b[0;36mtake_action\u001b[1;34m(self, info, env, brain_name, steps, normalize)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mnew_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_experiences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\pj5\\ml-agents\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action, memory, value)\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"STEP\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUnityEnvironmentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No Unity environment is loaded.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\pj5\\ml-agents\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_get_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_brains\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m             \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"brain_name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m             \u001b[0mn_agent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"agents\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\pj5\\ml-agents\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_get_state_dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \"\"\"\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"RECEIVED\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\pj5\\ml-agents\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m             \u001b[0mmessage_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "#config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config = config) as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/crawler\\model-2700000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/crawler\\model-2700000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 7 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 7 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 7 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
